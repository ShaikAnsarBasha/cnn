{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limited-adaptation",
   "metadata": {},
   "source": [
    "<h1>This is the blue print for all transfer learning techniques</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "printable-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recreational-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning is 1001 classes classification from 0 to 1000\n",
    "# they are trained on 1.4M images of size (224,224)\n",
    "image_size = np.array([224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coordinate-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "legal-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# to run the below code we get errors so we write above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aboriginal-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include_top=False removes the last dense layer which contain 1001 neurons\n",
    "# weights parameter will import the weights of vgg16 model which is already trained\n",
    "vgg = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "variable-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we no need to train the existing weights so we are making trainable parameter to False\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "faced-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are flattening the last layer output\n",
    "# after passing the images to all layers the last layer output we get we flatten it\n",
    "# suppose the last layer output is (9, 9, 2) after flattening we get 168 \n",
    "x = Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "advance-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 25088) dtype=float32 (created by layer 'flatten_1')>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after flattening we got 25088 as output\n",
    "# None means for every input of image we get 25088\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "patent-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after flattening the vgg16 layers we now add last layer with 5neurons for prediction\n",
    "## by freezing all the layers of vgg16\n",
    "prediction = Dense(5, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "basic-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 5) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cognitive-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 125445    \n",
      "=================================================================\n",
      "Total params: 14,840,133\n",
      "Trainable params: 125,445\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now finally we are creating model \n",
    "## by combining all flatten layers and prediction layer\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "model.summary()\n",
    "# from below we can see that trainable parameters are only 125445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "printable-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we should tell the model what optimization and loss function should be used\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "elementary-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3490 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# we use only train_datagen because we are creating samples from only one folder flower_photos\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# directory should contain subdirectories of classes each contain jpg or png images\n",
    "# target_size is the dimensions to which all images found inside subdirectories will be resized.\n",
    "# classes is the all subdirectories \n",
    "# class_mode='sparse' will assign an integer to the classes eg[0, 1, 2, 3, 4]\n",
    "## if class_mode is 'categorical' it will convert outputs to one_hot representation\n",
    "# batch_size is size of batches of data\n",
    "training_set = train_datagen.flow_from_directory(directory='datasets/train',\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=32,\n",
    "                                                 classes=['daisy', 'dandelion', 'roses', 'sunflowers',\n",
    "                                                          'tulips'],\n",
    "                                                 class_mode='sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "norwegian-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# if we are not using train_test_split then we use below code \n",
    "## because training set is different and testing set is different we use below code\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory('datasets/test',\n",
    "                                            target_size=(224, 224),\n",
    "                                            batch_size=32,\n",
    "                                            classes=['daisy', 'dandelion', 'roses', 'sunflowers',\n",
    "                                                     'tulips'],\n",
    "                                            class_mode='sparse')\n",
    "# we created test directory with 5 subdirectories\n",
    "## each subdirectory contains 36 images\n",
    "# so total images are 36*5=180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "latter-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(20, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(20, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n",
      "(20, 224, 224, 3)\n",
      "(32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# each element in test_set is tuple of 4d arrays and its corresponding ys\n",
    "i = 0\n",
    "for j in test_set:\n",
    "    print(j[0].shape)\n",
    "    i += 1\n",
    "    if i>18:\n",
    "        break\n",
    "# by observing the below pattern there are 32 5s and one 20 this sequence will repeat \n",
    "## so total 32*5 + 20 = 180\n",
    "# and the above pattern will repeat for infinite no of times so we are using break statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dying-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "[4. 0. 0. 3. 4. 0. 1. 3. 4. 3. 0. 3. 3. 4. 4. 4. 2. 1. 3. 4. 0. 0. 2. 0.\n",
      " 0. 3. 1. 1. 0. 1. 2. 3.]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# each element in test_set is tuple of 4d arrays and its corresponding ys\n",
    "for j in test_set:\n",
    "    print(j[0].shape)\n",
    "    print(j[1])\n",
    "    print(len(j[1]))\n",
    "    break\n",
    "# by observing the below pattern there are 32 5s and one 20 this sequence will repeat \n",
    "## so total 32*5 + 20 = 180\n",
    "# and the above pattern will repeat for infinite no of times so we are using break statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "married-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.preprocessing.image.DirectoryIterator"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "regulated-lemon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set) # 32 5s and 20 one so total 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "convenient-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "110/110 [==============================] - 164s 1s/step - loss: 1.1549 - accuracy: 0.5840 - val_loss: 0.8295 - val_accuracy: 0.7444\n",
      "Epoch 2/2\n",
      "110/110 [==============================] - 147s 1s/step - loss: 0.4798 - accuracy: 0.8436 - val_loss: 0.8978 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "ml = model.fit(training_set, validation_data=test_set, epochs=2,\n",
    "                                                  steps_per_epoch=len(training_set),\n",
    "                                                  validation_steps=len(test_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "industrial-parcel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.870847225189209, 0.477873831987381],\n",
       " 'accuracy': [0.6902579069137573, 0.8383954167366028],\n",
       " 'val_loss': [0.8295403122901917, 0.8978334069252014],\n",
       " 'val_accuracy': [0.7444444298744202, 0.7777777910232544]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
